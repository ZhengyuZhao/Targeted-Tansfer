{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_ground_truth(csv_filename):\n",
    "    image_id_list = []\n",
    "    label_ori_list = []\n",
    "    label_tar_list = []\n",
    "\n",
    "    with open(csv_filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            image_id_list.append( row['ImageId'] )\n",
    "            label_ori_list.append( int(row['TrueLabel'])-1 )\n",
    "            label_tar_list.append( int(row['TargetClass'])-1 )\n",
    "\n",
    "    return image_id_list,label_ori_list,label_tar_list\n",
    "\n",
    "\n",
    "# simple Module to normalize an image\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.Tensor(mean)\n",
    "        self.std = torch.Tensor(std)\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean.type_as(x)[None,:,None,None]) / self.std.type_as(x)[None,:,None,None]\n",
    "\n",
    "# values are standard normalization for ImageNet images, \n",
    "# from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "norm = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = transforms.Compose([\n",
    "#      transforms.Resize((256, 256)),\n",
    "     transforms.ToTensor(),])\n",
    "\n",
    "image_id_list,label_ori_list,label_tar_list=load_ground_truth(os.path.join('../', 'images.csv'))\n",
    "\n",
    "\n",
    "model_1 = models.inception_v3(pretrained=True,transform_input=True).eval()\n",
    "model_2 = models.resnet50(pretrained=True).eval()\n",
    "model_3 = models.densenet121(pretrained=True).eval()\n",
    "model_4 = models.vgg16_bn(pretrained=True).eval()\n",
    "\n",
    " \n",
    "for param in model_1.parameters():\n",
    "    param.requires_grad=False  \n",
    "for param in model_2.parameters():\n",
    "    param.requires_grad=False  \n",
    "for param in model_3.parameters():\n",
    "    param.requires_grad=False  \n",
    "for param in model_4.parameters():\n",
    "    param.requires_grad=False  \n",
    "\n",
    "    \n",
    "device  = torch.device(\"cuda:0\")\n",
    "model_1.to(device)\n",
    "model_2.to(device)\n",
    "model_3.to(device)\n",
    "model_4.to(device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "##define TI\n",
    "def gkern(kernlen=15, nsig=3):\n",
    "    x = np.linspace(-nsig, nsig, kernlen)\n",
    "    kern1d = st.norm.pdf(x)\n",
    "    kernel_raw = np.outer(kern1d, kern1d)\n",
    "    kernel = kernel_raw / kernel_raw.sum()\n",
    "    return kernel\n",
    "\n",
    "channels=3\n",
    "kernel_size=5\n",
    "kernel = gkern(kernel_size, 3).astype(np.float32)\n",
    "gaussian_kernel = np.stack([kernel, kernel, kernel])\n",
    "gaussian_kernel = np.expand_dims(gaussian_kernel, 1)\n",
    "gaussian_kernel = torch.from_numpy(gaussian_kernel).cuda()\n",
    "gaussian_filter = nn.Conv2d(in_channels=channels, out_channels=channels,\n",
    "                                 kernel_size=kernel_size, groups=channels, bias=False,padding=7)\n",
    "gaussian_filter.weight.data = gaussian_kernel\n",
    "\n",
    "\n",
    "\n",
    "##define DI\n",
    "def DI(X_in):\n",
    "    rnd=np.random.randint(299, 330,size=1)[0]\n",
    "    h_rem = 330 - rnd\n",
    "    w_rem = 330 - rnd\n",
    "    pad_top = np.random.randint(0, h_rem,size=1)[0]\n",
    "    pad_bottom = h_rem - pad_top\n",
    "    pad_left = np.random.randint(0, w_rem,size=1)[0]\n",
    "    pad_right = w_rem - pad_left\n",
    "\n",
    "    c=np.random.rand(1)\n",
    "    if c<=0.7:\n",
    "        X_out=F.pad(F.interpolate(X_in, size=(rnd,rnd)),(pad_left,pad_top,pad_right,pad_bottom),mode='constant', value=0)\n",
    "        return  X_out \n",
    "    else:\n",
    "        return  X_in\n",
    "    \n",
    "    \n",
    "## define Po+Trip\n",
    "def Poincare_dis(a, b):\n",
    "    L2_a = torch.sum(torch.square(a), 1)\n",
    "    L2_b = torch.sum(torch.square(b), 1)\n",
    "\n",
    "    theta = 2 * torch.sum(torch.square(a - b), 1) / ((1 - L2_a) * (1 - L2_b))\n",
    "    distance = torch.mean(torch.acosh(1.0 + theta))\n",
    "    return distance\n",
    "\n",
    "def Cos_dis(a, b):\n",
    "    a_b = torch.abs(torch.sum(torch.multiply(a, b), 1))\n",
    "    L2_a = torch.sum(torch.square(a), 1)\n",
    "    L2_b = torch.sum(torch.square(b), 1)\n",
    "    distance = torch.mean(a_b / torch.sqrt(L2_a * L2_b))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-7b819d42cfe3>:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for k in tqdm_notebook(range(0,num_batches)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adc1183108445b4a3426801db64e666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7b819d42cfe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#DI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_ori\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlogit_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/depdisk/anaconda3/envs/inv3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c49226c9d480>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# values are standard normalization for ImageNet images,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Single model transfer setting, from ResNet 50 to other models.\n",
    "## Test with three different losses: our logit, Po+Trip, and CE.'-TMDI' is used.\n",
    "batch_size=20\n",
    "max_iterations=300\n",
    "input_path='../images/'\n",
    "num_batches = np.int(np.ceil(len(image_id_list)/batch_size))\n",
    "img_size=299\n",
    "lr=2/255\n",
    "epsilon=16\n",
    "\n",
    "\n",
    "#logit\n",
    "pos=np.zeros((3,max_iterations//20))\n",
    "for k in tqdm_notebook(range(0,num_batches)):\n",
    "    batch_size_cur=min(batch_size,len(image_id_list)-k*batch_size)        \n",
    "    X_ori = torch.zeros(batch_size_cur,3,img_size,img_size).to(device)\n",
    "    delta= torch.zeros_like(X_ori,requires_grad=True).to(device)\n",
    "    for i in range(batch_size_cur):          \n",
    "        X_ori[i]=trn(Image.open(input_path+image_id_list[k*batch_size+i]+'.png'))  \n",
    "\n",
    "    labels=torch.tensor(label_tar_list[k*batch_size:k*batch_size+batch_size_cur]).to(device)\n",
    "    grad_pre=0\n",
    "    prev = float('inf')\n",
    "    for t in range(max_iterations):\n",
    "        #DI\n",
    "        logits = model_2(norm(DI(X_ori+delta)))\n",
    "        real = logits.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "        logit_dists = ( -1*real)\n",
    "        loss=logit_dists.sum()\n",
    "        loss.backward()\n",
    "\n",
    "        #MI+TI   \n",
    "        grad_c=delta.grad.clone()\n",
    "        grad_c=F.conv2d(grad_c, gaussian_kernel, bias=None, stride=1, padding=(2,2), groups=3) \n",
    "        grad_a = grad_c +1*grad_pre\n",
    "        grad_pre=grad_a            \n",
    "        delta.grad.zero_()\n",
    "\n",
    "        delta.data=delta.data-lr* torch.sign(grad_a)\n",
    "        delta.data=delta.data.clamp(-epsilon/255,epsilon/255) \n",
    "        delta.data=((X_ori+delta.data).clamp(0,1))-X_ori\n",
    "        if t % 20 == 19:\n",
    "            pos[0,t // 20]=pos[0,t // 20]+sum(torch.argmax(model_1(norm(X_ori+delta)),dim=1)==labels).cpu().numpy()\n",
    "            pos[1,t // 20]=pos[1,t // 20]+sum(torch.argmax(model_3(norm(X_ori+delta)),dim=1)==labels).cpu().numpy()\n",
    "            pos[2,t // 20]=pos[2,t // 20]+sum(torch.argmax(model_4(norm(X_ori+delta)),dim=1)==labels).cpu().numpy()\n",
    "\n",
    "torch.cuda.empty_cache()    \n",
    "pos_res50_logit=np.copy(pos)\n",
    "\n",
    "#Po+Trip\n",
    "pos=np.zeros((3,max_iterations//20))\n",
    "for k in tqdm_notebook(range(0,num_batches)):\n",
    "    batch_size_cur=min(batch_size,len(image_id_list)-k*batch_size)        \n",
    "    X_ori = torch.zeros(batch_size_cur,3,img_size,img_size).to(device)\n",
    "    delta= torch.zeros_like(X_ori,requires_grad=True).to(device)\n",
    "    for i in range(batch_size_cur):          \n",
    "        X_ori[i]=trn(Image.open(input_path+image_id_list[k*batch_size+i]+'.png'))  \n",
    "\n",
    "    labels=torch.tensor(label_tar_list[k*batch_size:k*batch_size+batch_size_cur]).to(device)\n",
    "    labels_true=torch.tensor(label_ori_list[k*batch_size:k*batch_size+batch_size_cur]).to(device)\n",
    "    labels_onehot = torch.zeros(batch_size_cur, 1000, device=device)\n",
    "    labels_onehot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "    labels_true_onehot = torch.zeros(batch_size_cur, 1000, device=device)\n",
    "    labels_true_onehot.scatter_(1, labels_true.unsqueeze(1), 1)\n",
    "    labels_infhot = torch.zeros_like(labels_onehot).scatter_(1, labels.unsqueeze(1), float('inf')) \n",
    "    grad_pre=0\n",
    "    prev = float('inf')\n",
    "    for t in range(max_iterations):\n",
    "        logits = model_2(norm(DI(X_ori+delta)))\n",
    "\n",
    "        loss_po = Poincare_dis(logits / torch.sum(torch.abs(logits), 1, keepdim=True),torch.clamp((labels_onehot - 0.00001), 0.0, 1.0))\n",
    "        loss_cos = torch.clamp(Cos_dis(labels_onehot, logits) - Cos_dis(labels_true_onehot, logits) + 0.007, 0.0, 2.1)\n",
    "        loss=loss_po+0.01*loss_cos\n",
    "        loss.backward()\n",
    "\n",
    "        #MI+TI\n",
    "        grad_c=delta.grad.clone()\n",
    "        grad_c=F.conv2d(grad_c, gaussian_kernel, bias=None, stride=1, padding=(2,2), groups=3) \n",
    "        grad_a = grad_c +1*grad_pre\n",
    "        grad_pre=grad_a            \n",
    "        delta.grad.zero_()\n",
    "\n",
    "        delta.data=delta.data-lr* torch.sign(grad_a)\n",
    "        delta.data=delta.data.clamp(-epsilon/255,epsilon/255) \n",
    "        delta.data=((X_ori+delta.data).clamp(0,1))-X_ori\n",
    "        if t % 20 == 19:\n",
    "            pos[0,t // 20]=pos[0,t // 20]+sum(torch.argmax(model_1(norm(X_ori+delta)),dim=1)==labels).cpu().numpy()\n",
    "            pos[1,t // 20]=pos[1,t // 20]+sum(torch.argmax(model_3(norm(X_ori+delta)),dim=1)==labels).cpu().numpy()\n",
    "            pos[2,t // 20]=pos[2,t // 20]+sum(torch.argmax(model_4(norm(X_ori+delta)),dim=1)==labels).cpu().numpy()\n",
    "torch.cuda.empty_cache()    \n",
    "pos_res50_trip_po=np.copy(pos)\n",
    "\n",
    "\n",
    "\n",
    "# CE\n",
    "pos=np.zeros((3,max_iterations//20))\n",
    "for k in tqdm_notebook(range(0,num_batches)):\n",
    "    batch_size_cur=min(batch_size,len(image_id_list)-k*batch_size)        \n",
    "    X_ori = torch.zeros(batch_size_cur,3,img_size,img_size).to(device)\n",
    "    delta= torch.zeros_like(X_ori,requires_grad=True).to(device)\n",
    "    for i in range(batch_size_cur):          \n",
    "        X_ori[i]=trn(Image.open(input_path+image_id_list[k*batch_size+i]+'.png'))  \n",
    "\n",
    "    labels=torch.tensor(label_tar_list[k*batch_size:k*batch_size+batch_size_cur]).to(device)\n",
    "    grad_pre=0\n",
    "    prev = float('inf')\n",
    "    for t in range(max_iterations):\n",
    "        logits = model_2(norm(DI(X_ori+delta)))\n",
    "        loss = nn.CrossEntropyLoss(reduction='sum')(logits,labels)\n",
    "        loss.backward()\n",
    "\n",
    "        #MI + TI  \n",
    "        grad_c=delta.grad.clone()\n",
    "        grad_c=F.conv2d(grad_c, gaussian_kernel, bias=None, stride=1, padding=(2,2), groups=3) \n",
    "        grad_a = grad_c / torch.mean(torch.abs(grad_c), (1, 2, 3), keepdim=True)+1*grad_pre\n",
    "        grad_pre=grad_a            \n",
    "        delta.grad.zero_()\n",
    "\n",
    "        delta.data=delta.data-lr* torch.sign(grad_a)\n",
    "        delta.data=delta.data.clamp(-epsilon/255,epsilon/255) \n",
    "        delta.data=((X_ori+delta.data).clamp(0,1))-X_ori\n",
    "        if t % 20 == 19:\n",
    "            pos[0,t // 20]=pos[0,t // 20]+sum(torch.argmax(model_1(norm(X_ori+delta)),dim=1)==labels).cpu().numpy()\n",
    "            pos[1,t // 20]=pos[1,t // 20]+sum(torch.argmax(model_3(norm(X_ori+delta)),dim=1)==labels).cpu().numpy()\n",
    "            pos[2,t // 20]=pos[2,t // 20]+sum(torch.argmax(model_4(norm(X_ori+delta)),dim=1)==labels).cpu().numpy()\n",
    "\n",
    "torch.cuda.empty_cache()    \n",
    "pos_res50_ce=np.copy(pos)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensemble transfer from four different white-box models.\n",
    "## Test with our logit loss +'-TMDI'.\n",
    "batch_size=8\n",
    "max_iterations=300\n",
    "input_path='./images/'\n",
    "num_batches = np.int(np.ceil(len(image_id_list)/batch_size))\n",
    "img_size=299\n",
    "lr=2/255\n",
    "epsilon=16\n",
    "\n",
    "#logit\n",
    "for kk in [20,100,299]:\n",
    "    output_path='Target_adv_images_ens4_logit'+'_'+str(kk)+'/'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "for k in tqdm_notebook(range(0,num_batches)):\n",
    "    batch_size_cur=min(batch_size,len(image_id_list)-k*batch_size)        \n",
    "    X_ori = torch.zeros(batch_size_cur,3,img_size,img_size).to(device)\n",
    "    delta= torch.zeros_like(X_ori,requires_grad=True).to(device)\n",
    "    for i in range(batch_size_cur):          \n",
    "        X_ori[i]=trn(Image.open(input_path+image_id_list[k*batch_size+i]+'.png'))  \n",
    "    labels=torch.tensor(label_tar_list[k*batch_size:k*batch_size+batch_size_cur]).to(device)\n",
    "    grad_pre=0\n",
    "    prev = float('inf')\n",
    "    for t in range(max_iterations):\n",
    "        logits = (model_1(norm(DI(X_ori+delta)))+model_2(norm(DI(X_ori+delta)))+model_3(norm(DI(X_ori+delta)))+model_4(norm(DI(X_ori+delta))))/4\n",
    "        real = logits.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "        logit_dists = ( -1*real)\n",
    "        loss=logit_dists.sum()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        #MI+TI   \n",
    "        grad_c=delta.grad.clone()\n",
    "        grad_c=F.conv2d(grad_c, gaussian_kernel, bias=None, stride=1, padding=(2,2), groups=3) \n",
    "        grad_a = grad_c+1*grad_pre\n",
    "\n",
    "        grad_pre=grad_a            \n",
    "        delta.grad.zero_()\n",
    "\n",
    "        delta.data=delta.data-lr* torch.sign(grad_a)\n",
    "        delta.data=delta.data.clamp(-epsilon/255,epsilon/255) \n",
    "        delta.data=((X_ori+delta.data).clamp(0,1))-X_ori\n",
    "        if t == 20  or t == 100 or t == 299:\n",
    "            for j in range(batch_size_cur):\n",
    "                x_np=transforms.ToPILImage()((X_ori+delta)[j].detach().cpu())\n",
    "                x_np.save(os.path.join('Target_adv_images_ens4_logit'+'_'+str(t)+'/',image_id_list[k*batch_size+j]))\n",
    "torch.cuda.empty_cache()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create targeted UAP\n",
    "batch_size=20\n",
    "max_iterations=300\n",
    "img_size=299\n",
    "lr=2/255\n",
    "epsilon=16\n",
    "# Creating UAP for all 1000 ImageNet classes from 0 to 999\n",
    "num_batches=1000//batch_size\n",
    "\n",
    "#logit\n",
    "output_path='UAP_vgg16_logit/'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "for k in tqdm_notebook(range(0,num_batches)):\n",
    "    #starting from a pseudo image with all pixel values as 0.5\n",
    "    X_ori = torch.full((batch_size,3,img_size,img_size),0.5).to(device)\n",
    "    delta= torch.zeros_like(X_ori,requires_grad=True).to(device)\n",
    "    \n",
    "    #assign target labels\n",
    "    labels=torch.tensor(np.arange(batch_size)+k*batch_size).to(device)\n",
    "    grad_pre=0\n",
    "    prev = float('inf')\n",
    "    for t in range(max_iterations):\n",
    "        #DI\n",
    "        logits = model_4(norm(DI(X_ori+delta)))\n",
    "        real = logits.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "        logit_dists = ( -1*real)\n",
    "        loss=logit_dists.sum()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        #MI+TI   \n",
    "        grad_c=delta.grad.clone()\n",
    "        grad_c=F.conv2d(grad_c, gaussian_kernel, bias=None, stride=1, padding=(2,2), groups=3) \n",
    "        grad_a = grad_c+1*grad_pre\n",
    "\n",
    "        grad_pre=grad_a            \n",
    "        delta.grad.zero_()\n",
    "\n",
    "        delta.data=delta.data-lr* torch.sign(grad_a)\n",
    "        delta.data=delta.data.clamp(-epsilon/255,epsilon/255) \n",
    "#         delta.data=((X_ori+delta.data).clamp(0,1))-X_ori\n",
    "    for j in range(batch_size):\n",
    "        x_np=transforms.ToPILImage()((X_ori+delta)[j].detach().cpu())\n",
    "        x_np.save(output_path+str(k*batch_size+j)+'.png')\n",
    "torch.cuda.empty_cache()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluate targeted UAP\n",
    "input_path='./images/'\n",
    "batch_size=100\n",
    "num_batches=len(image_id_list)//batch_size\n",
    "img_size=299\n",
    "per_path='./UAP_vgg16_logit/'\n",
    "pos=np.zeros(1000)\n",
    "for label in tqdm_notebook(range(0,1000)):    \n",
    "    for k in range(0,num_batches):\n",
    "        X_ori = torch.zeros(batch_size,3,img_size,img_size).to(device)\n",
    "        for i in range(batch_size):          \n",
    "            X_ori[i]=trn(Image.open(input_path+image_id_list[k*batch_size+i]+'.png'))           \n",
    "        X_per=trn(Image.open(per_path+str(label)+'.png')).to(device)\n",
    "        X_adv=(X_ori+X_per-0.5).clamp(0,1)\n",
    "        #save the number of sucessful examples for each target label\n",
    "        pos[label]=pos[label]+sum(torch.argmax(model_4(norm(X_adv)),dim=1)==label).cpu().numpy()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
